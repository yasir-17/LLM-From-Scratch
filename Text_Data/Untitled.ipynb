{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd758053-0f27-4b3e-a066-7a470e201287",
   "metadata": {},
   "source": [
    "# Working with Text\n",
    "1) Text Embeddings.\n",
    "2) Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e96a2454-3b7a-47cc-83bf-bfdb447c5942",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23875aff-28ac-4ed2-b58f-60b61c17e901",
   "metadata": {},
   "source": [
    "Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c785c168-e8b8-467f-acb8-08cfddef392f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/y.khan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "preprocessed = nltk.word_tokenize(raw_text)\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c226edac-a0d8-4999-b30f-739fdd4175ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4544\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48007cad-6d5f-467d-b834-cedff83a4706",
   "metadata": {},
   "source": [
    "# Convert tokens to tokens ID\n",
    "1) The tokens are mapped to token id that we can process via embedding layers later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5143f322-5160-47cf-bc44-e3455315aa23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3025db2f-4ff8-467b-903c-befef11121b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e81c2-6083-4c89-93c4-f89d965a4663",
   "metadata": {},
   "source": [
    "# Create tokenizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf39a8f-e058-42e8-93ec-7edd87b2a2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = nltk.word_tokenize(text)\n",
    "        ids = [self.vocab[s] for s in preprocessed if s in self.vocab]\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        tokens = [list(self.vocab.keys())[list(self.vocab.values()).index(i)] for i in ids]\n",
    "        return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4db6c460-f74c-48af-9086-a3d8a6f5d9d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67, 10, 999, 618, 550, 762, 17, 1134, 612, 17, 78, 49, 864, 1117, 770, 809, 19]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer(vocab)\n",
    "\n",
    "text = \"It's the last he painted, you know, Mrs. Gisburn said with pardonable pride.\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bd8adef-c850-4c9f-a405-db196fb96733",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It 's the last he painted , you know , Mrs. Gisburn said with pardonable pride .\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47fce783-91f0-40a1-b52a-65337f94d1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It 's the last he painted , you know , Mrs. Gisburn said with pardonable pride .\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c53a29-e555-477d-80a0-c51f6ea1891a",
   "metadata": {},
   "source": [
    "# Adding Special Context Tokens\n",
    "\n",
    "1) [BOS] - Beginning of sentence.\n",
    "2) [EOS] - End of sentence. This usually use to concatenate two different article, etc.\n",
    "3) [PAD] - If we train LLM on batch size greater than 1. The two sentences can have different lengths. Padding is used to pad sentence of shorter length\n",
    "4) [UNK] - Represent word that are not included in the vocabulary\n",
    "\n",
    "5) GPT-2 does not need any of these tokens mentioned above but only uses an <|endoftext|> token to reduce complexity.\n",
    "6) GPT also uses the <|endoftext|> for padding (since we typically use a mask when training on batched inputs, we would not attend padded tokens anyways, so it does not matter what these tokens are)\n",
    "7) GPT-2 does not use <UNK>. Instead it uses byte-pair encoding (BPE) to break down words into subword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9a8660e-4449-4692-bc7a-1cec95ac2da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d8ac23e-b8ea-4b06-82e0-5e88a92703ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.special_tokens = [\"<|endoftext|>\", \"<|unk|>\"]\n",
    "        \n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.vocab\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.vocab.get(item, self.vocab[\"<|unk|>\"]) for item in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        rev_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        text = \" \".join([rev_vocab.get(i, \"<|unk|>\") for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "634b53f5-5525-4ce7-91d8-bb12d034c67b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordTokenizer(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7b3b4c-eec7-4b1c-bab4-1f33b2f43e1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1139, 17, 377, 1134, 644, 987, 22, 1138, 66, 999, 969, 995, 738, 999, 1139, 19]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4146d73-9cfa-454d-bc76-87646a30fa85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e5ad3b-7a8f-466e-b9bc-23969e1f52f3",
   "metadata": {},
   "source": [
    "# Byte pair encoding\n",
    "\n",
    "1) Handles out of vocabolary situation by breaking down words into smaller words or individual character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c7ce3c0-2145-4ff2-81d6-670ad3abf3a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d63b77-9203-4adc-a7ff-c4b03a41e80f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb935cb3-4b31-4370-a7d9-aedcbb4af378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7c0fcd8-f511-4b18-b5d5-5d522aecc09e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f9a7e54-f4a0-43c9-aada-d18703a9958d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f20ab7-c481-4584-8673-ebbaf1e97c28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Sampling with sliding window\n",
    "\n",
    "1) We train LLM to predict one token at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dff7efef-aa2e-4507-9a75-e86e0c1b3182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ee6c341-5a1d-475b-b090-9f6190209528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class GPTDataset(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d9c8666-aa20-43af-a19a-d610c7fc571b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataloader(text, batch_size = 12, max_length = 256, stride = 128, shuffle=True, drop_last=True, num_worker = 0):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataset(text, tokenizer, max_length, stride)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_worker\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8682d98-69c0-44ef-b45a-58124a85d0f4",
   "metadata": {},
   "source": [
    "Testing Dataloader with batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5dc8388-2bd9-4fe3-958b-8fde4e54451f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "dataloader = create_dataloader(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d934d-2ff9-4614-9301-e3bf56024954",
   "metadata": {},
   "source": [
    "Creating Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3f12b97-becd-462c-afcf-48eb7881da2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5292be6-8482-4a08-92fb-cdc24fcc630a",
   "metadata": {},
   "source": [
    "# Create Token Embeddings\n",
    "\n",
    "1) Embed the tokens into continuous vector representations using an embedding layer.\n",
    "2) Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training\n",
    "\n",
    "# Positional Embeddings\n",
    "1) Token embeddings are added with the positional embeddings to form the input embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "486c3055-34c4-4c0f-add1-5841626c9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e016d35b-3480-4f0a-bc28-258164f1d0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "batch_size = 8\n",
    "\n",
    "dataloader = create_dataloader(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0c15cf6-bc4e-4b72-937d-6df95b6ff1fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142324e4-3c63-49e3-b5b9-c3d1c7c9d0fa",
   "metadata": {},
   "source": [
    "GPT2 uses absolute positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25ec431c-b312-4b8a-bf9f-ccd2fe96fffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06a0ef12-8588-41c4-aef2-ab676c96101d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28aaa126-f1b5-4496-be0e-80ee5926c277",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f730a-2aa0-4f06-89f6-aa179860a79f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-llm",
   "language": "python",
   "name": "hf-llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
